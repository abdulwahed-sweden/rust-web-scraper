# ğŸ—ï¸ Architecture Documentation

## Overview

Rust Web Scraper is built with a clean, modular architecture that separates concerns and makes the codebase maintainable and extensible.

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      User Interfaces                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Web Browser     â”‚   Tauri Desktop  â”‚   API Clients        â”‚
â”‚   (static/*)      â”‚   (wrapper)      â”‚   (REST API)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Actix-Web Server                         â”‚
â”‚                      (src/main.rs)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ HTTP Server                                              â”‚
â”‚  â€¢ Static File Serving                                      â”‚
â”‚  â€¢ CORS Configuration                                       â”‚
â”‚  â€¢ Logging & Compression                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     API Layer                               â”‚
â”‚                    (src/api.rs)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Route Handlers                                           â”‚
â”‚  â€¢ Request/Response Models                                  â”‚
â”‚  â€¢ Session Management                                       â”‚
â”‚  â€¢ Error Handling                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Scraping Engine                            â”‚
â”‚                 (src/scraper.rs)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ WebScraper - Main scraping logic                         â”‚
â”‚  â€¢ Configuration management                                 â”‚
â”‚  â€¢ Pagination handling                                      â”‚
â”‚  â€¢ Result aggregation                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                           â”‚
          â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Auto Selectors     â”‚    â”‚     Utilities                â”‚
â”‚ (auto_selectors.rs) â”‚    â”‚     (utils.rs)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ SelectorDetector  â”‚    â”‚ â€¢ RateLimiter                â”‚
â”‚ â€¢ Content extractionâ”‚    â”‚ â€¢ User Agent rotation        â”‚
â”‚ â€¢ Smart heuristics  â”‚    â”‚ â€¢ Helper functions           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Module Breakdown

### 1. Main Server (`src/main.rs`)

**Purpose**: Entry point and HTTP server configuration

**Responsibilities**:
- Initialize Actix-Web HTTP server
- Configure middleware (CORS, logging, compression)
- Define API routes
- Serve static files
- Manage application state

**Key Components**:
```rust
AppState {
    sessions: Arc<Mutex<Vec<ScrapingSession>>>
}
```

### 2. API Layer (`src/api.rs`)

**Purpose**: HTTP request handling and API endpoints

**Endpoints**:
- `GET /api/health` - Health check
- `POST /api/scrape` - Start scraping job
- `GET /api/sessions` - Get all sessions
- `GET /api/sessions/{id}` - Get specific session
- `DELETE /api/sessions` - Clear all sessions

**Models**:
- `ScrapeRequest` - Input parameters
- `ScrapeResponse` - API response wrapper
- `AppState` - Shared application state

### 3. Scraping Engine (`src/scraper.rs`)

**Purpose**: Core scraping logic and orchestration

**Key Types**:
```rust
pub struct WebScraper {
    client: reqwest::Client,
    rate_limiter: RateLimiter,
    detector: SelectorDetector,
    verbose: bool,
}

pub struct ScrapingConfig {
    urls: Vec<String>,
    enable_pagination: bool,
    max_pages: usize,
    rate_limit: f64,
    custom_selectors: Option<AutoSelectors>,
}

pub struct ScrapingSession {
    start_time: String,
    config: ScrapingConfig,
    results: Vec<ScrapingResult>,
    total_pages_scraped: usize,
    total_links_found: usize,
    total_images_found: usize,
    errors: Vec<String>,
}
```

**Flow**:
1. Create `WebScraper` with configuration
2. For each URL:
   - Fetch page with rate limiting
   - Detect/apply selectors
   - Extract content
   - If pagination enabled, find next page
3. Aggregate results into `ScrapingSession`

### 4. Auto Selectors (`src/auto_selectors.rs`)

**Purpose**: Intelligent content detection without manual configuration

**Key Types**:
```rust
pub struct SelectorDetector {
    selectors: AutoSelectors,
}

pub struct AutoSelectors {
    title: Vec<String>,
    content: Vec<String>,
    links: Vec<String>,
    images: Vec<String>,
    metadata: Vec<String>,
}

pub struct DetectedContent {
    title: Option<String>,
    content: Vec<String>,
    links: Vec<LinkData>,
    images: Vec<ImageData>,
    metadata: HashMap<String, String>,
}
```

**Detection Strategy**:
1. **Title**: Try common patterns (h1, title, og:title)
2. **Content**: Look for article, main, paragraphs
3. **Links**: Extract all anchor tags with URLs
4. **Images**: Find img tags and data-src attributes
5. **Metadata**: Parse meta tags for SEO data

**Smart Features**:
- Duplicate detection
- URL resolution (relative â†’ absolute)
- External link detection
- Empty content filtering

### 5. Utilities (`src/utils.rs`)

**Purpose**: Shared utilities and helpers

**Components**:
- `RateLimiter`: Polite scraping with delays
- `get_random_user_agent()`: Rotate user agents
- Constants and helper functions

## Data Flow

### Scraping Request Flow

```
1. User submits form in Web UI
   â””â”€> POST /api/scrape with JSON body

2. API handler validates request
   â””â”€> Creates ScrapingConfig

3. WebScraper initialized
   â””â”€> Creates HTTP client
   â””â”€> Sets up rate limiter
   â””â”€> Configures selector detector

4. For each URL:
   â””â”€> Rate limiter waits
   â””â”€> Fetch page (reqwest)
   â””â”€> Parse HTML (scraper crate)
   â””â”€> Detect/extract content
   â””â”€> Find next page if pagination enabled
   â””â”€> Store ScrapingResult

5. Aggregate results
   â””â”€> Create ScrapingSession
   â””â”€> Store in AppState
   â””â”€> Return to client

6. Client displays results
   â””â”€> Show statistics
   â””â”€> Render extracted content
   â””â”€> Offer JSON download
```

## Technology Stack

### Backend
- **Actix-Web** - Fast, ergonomic web framework
- **Tokio** - Async runtime
- **Reqwest** - HTTP client
- **Scraper** - HTML parsing with CSS selectors
- **Serde** - Serialization/deserialization
- **Anyhow** - Error handling

### Frontend
- **Vanilla JavaScript** - No framework overhead
- **Modern CSS** - Grid, Flexbox, CSS Variables
- **Responsive Design** - Works on all devices

### DevOps
- **Docker** - Containerization
- **Docker Compose** - Multi-container orchestration
- **Cargo** - Build system and package manager

## Design Patterns

### 1. Builder Pattern
Used in `WebScraper::new()` and configuration

### 2. Strategy Pattern
`SelectorDetector` can use custom or auto selectors

### 3. Repository Pattern
`AppState` manages session storage

### 4. Middleware Chain
Actix-Web middleware for logging, CORS, compression

### 5. Async/Await
Throughout for non-blocking I/O

## Error Handling

- Uses `anyhow::Result` for flexible error propagation
- Graceful degradation (continues on individual URL failures)
- Detailed error messages logged
- User-friendly error responses in API

## Performance Considerations

### Async Operations
- All network I/O is async
- Non-blocking HTTP server
- Concurrent URL processing (when not paginating)

### Rate Limiting
- Prevents overwhelming target servers
- Configurable requests per second
- Automatic delays between requests

### Memory Efficiency
- Streaming HTML parsing
- Bounded session storage
- Efficient string handling

### Optimization Opportunities
- Connection pooling (reqwest handles this)
- Response compression
- Caching (future enhancement)

## Security Considerations

### CORS
- Configured to allow any origin (adjust for production)

### Input Validation
- URL validation
- Selector syntax validation
- Rate limit bounds

### Rate Limiting
- Prevents abuse of scraping service
- Protects target servers

### No Authentication (Current)
- Add auth for production use
- Consider API keys
- Implement rate limiting per user

## Scalability

### Horizontal Scaling
- Stateless design (sessions stored in memory, can move to Redis)
- Docker-friendly
- Can run multiple instances behind load balancer

### Vertical Scaling
- Efficient async I/O
- Low memory footprint
- Fast HTML parsing

### Database Integration
- Currently in-memory storage
- Easy to add PostgreSQL/MongoDB for persistence
- Sessions are serializable (JSON)

## Future Enhancements

### Backend
- [ ] Persistent storage (PostgreSQL/Redis)
- [ ] WebSocket for real-time updates
- [ ] Job queue (for long-running scrapes)
- [ ] Authentication & authorization
- [ ] User accounts and saved configurations
- [ ] JavaScript rendering (headless browser)
- [ ] Proxy support

### Frontend
- [ ] Real-time progress updates
- [ ] Export to CSV/Excel
- [ ] Saved scraping templates
- [ ] Schedule recurring scrapes
- [ ] Data visualization charts

### Architecture
- [ ] Microservices (separate scraping workers)
- [ ] Message queue (RabbitMQ/Kafka)
- [ ] Caching layer (Redis)
- [ ] Monitoring & alerting (Prometheus/Grafana)

## Testing Strategy

### Unit Tests
- Test individual functions
- Mock HTTP responses
- Validate selector detection

### Integration Tests
- Test API endpoints
- Verify end-to-end flows
- Test with real websites (carefully)

### Performance Tests
- Load testing with Apache Bench
- Memory profiling
- Concurrent request handling

## Deployment

### Local Development
```bash
cargo run
```

### Docker
```bash
docker build -t rust-web-scraper .
docker run -p 8080:8080 rust-web-scraper
```

### Docker Compose
```bash
docker-compose up
```

### Production Considerations
- Use reverse proxy (Nginx)
- Enable HTTPS (Let's Encrypt)
- Set up monitoring
- Configure log rotation
- Implement backup strategy

---

**Last Updated**: 2025-10-30
**Version**: 0.3.0
